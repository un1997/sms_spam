# -*- coding: utf-8 -*-
"""SMS_Spam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hW9T1kHUsdX6nkp03HO9FqcOdvaxqk3d

**SMS Spam Classification Dataset from the UCI Repository**
"""

import pandas as pd
import requests
import io
    
# Downloading the csv file from your GitHub account

url = "https://raw.githubusercontent.com/un1997/sms_spam/main/SMSSpamCollection" # Make sure the url is the raw version of the file on GitHub
download = requests.get(url).content

# Reading the downloaded content and turning it into a pandas dataframe
df = pd.read_csv(io.BytesIO(download),sep='\t',names=['label','message'])

# Printing out the first 5 rows of the dataframe

df.head()

"""**Import necessary Packages**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy as sp
from sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support as score
# %matplotlib inline

"""**Read the data using pandas framework with column names label and message**"""

dataset = df

"""Display first five rows from the dataset"""

dataset.head()

"""**Based on the labels, the number of ham & spam messages are counted and plotted**"""

dataset.groupby('label').describe()

count_Class=pd.value_counts(dataset["label"], sort= True)
count_Class.plot(kind = 'bar',color = ["green","red"])
plt.title('Bar Plot')
plt.show()

"""**Now the labels are given a numeric value 0 & 1.**

**SPAM : 1**

**HAM : 0**
"""

dataset['label'] = dataset['label'].map( {'spam': 1, 'ham': 0} )

dataset.head()

"""**Put different Labels in separate pandas dataframe inorder to generate WORDCLOUD**"""

data_ham  = dataset[dataset['label'] == 0].copy()
data_spam = dataset[dataset['label'] == 1].copy()

import wordcloud
from wordcloud import WordCloud
def show_wordcloud(df, title):
    text = ' '.join(df['message'].astype(str).tolist())
    stopwords = set(wordcloud.STOPWORDS)
    
    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='lightgrey',
                    colormap='viridis', width=800, height=600).generate(text)
    
    plt.figure(figsize=(10,7), frameon=True)
    plt.imshow(fig_wordcloud)  
    plt.axis('off')
    plt.title(title, fontsize=20 )
    plt.show()

show_wordcloud(data_spam, "Spam messages")

show_wordcloud(data_ham, "Ham messages")

"""**Text preprocessing & Training**"""

# helps in text preprocessing
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

# helps in model building
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import SimpleRNN
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Embedding
from tensorflow.keras.callbacks import EarlyStopping

# split data into train and test set
from sklearn.model_selection import train_test_split

"""**Dataset split into Train & Test**"""

X = dataset['message'].values
y = dataset['label'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

"""**Text Pre-Processing**

**1. Tokenization**

**2. Text Encoding**

**3. Padding**
"""

t = Tokenizer()
t.fit_on_texts(X_train)

encoded_train = t.texts_to_sequences(X_train)
encoded_test = t.texts_to_sequences(X_test)
print(X_train[0])
print(encoded_train[0])

max_length = 8
padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')
padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')
print(padded_train)

"""**Model is Built with vocabulary size as the input size.** 

**Model is compiled and summary generated**
"""

vocab_size = len(t.word_index) + 1

# define the model
model = Sequential()
model.add(Embedding(vocab_size, 24, input_length=max_length))
model.add(SimpleRNN(24, return_sequences=False))
model.add(Dense(1, activation='sigmoid'))

# compile the model
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

# summarize the model
print(model.summary())

"""**Model is trained and validated for test dataset with 50 epochs.**

**Callback is made at an early stage when the validation loss has its first minimum value.**
"""

early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)

# fit the model
model.fit(x=padded_train,
         y=y_train,
         epochs=100,
         validation_data=(padded_test, y_test), verbose=1,
         callbacks=[early_stop]
         )

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

def c_report(y_true, y_pred):
   print("Classification Report")
   print(classification_report(y_true, y_pred))
   acc_sc = accuracy_score(y_true, y_pred)
   print("Accuracy : "+ str(acc_sc))
   return acc_sc

def plot_confusion_matrix(y_true, y_pred):
   mtx = confusion_matrix(y_true, y_pred)
   sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5, 
               cmap="Blues", cbar=False)
   plt.ylabel('True label')
   plt.xlabel('Predicted label')

"""**Model Predicted for test dataset.**

**Classification report and Confusion Matrix generated** 
"""

preds = (model.predict(padded_test) > 0.5).astype("int32")

c_report(y_test, preds)

plot_confusion_matrix(y_test, preds)

"""**Save the model and the tokenizer (picle package helps us to save the tokenizer to use it on new messages)**"""

model.save("spam_model")

import pickle
with open('spam_model/tokenizer.pkl', 'wb') as output:
   pickle.dump(t, output, pickle.HIGHEST_PROTOCOL)

"""**Load the model and the tokenizer and predict whether the new message is a spam / ham.**"""

import tensorflow as tf

s_model = tf.keras.models.load_model("spam_model")
with open('spam_model/tokenizer.pkl', 'rb') as input:
    tokenizer = pickle.load(input)

from tensorflow.keras.preprocessing.text import Tokenizer

# sms = ["Lottery won. You are given a free credit card offer with no emi for first month. call 7654437862 to claim your offer"]
sms = ["hello! how are you? im visiting mom next week"]
sms_proc = t.texts_to_sequences(sms)
sms_proc = pad_sequences(sms_proc, maxlen=max_length, padding='post')
pred = (model.predict(sms_proc)>0.5).astype("int32").item()
print(pred)

"""The sms "we offer a free loan. Call 9998846756 to avail the free loan" is predicted as spam with label 1. This is done based on the common words seen in spam wordcloud like offer, free, call. The Prediction accuracy is 98.3% as in classification report generated. [link text](https:://)

Reference Links:  
https://keras.io/api/callbacks/early_stopping/  
https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/  
https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/  
https://www.datatechnotes.com/2018/12/rnn-example-with-keras-simplernn-in.html   
https://medium.com/nerd-for-tech/building-a-text-classifier-using-rnn-57b546d3d35a
"""